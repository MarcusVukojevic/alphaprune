Inizio traning con:  {'name_model': 'meta-llama/Llama-2-7b-hf', 'eightbit': False, 'name_dataset': 'wikitext', 'device': 'cuda', 'target_sparsity': 0.3, 'ppl_tolerance_frac': 0.2, 'R_limit': 60, 'num_searches': 64, 'top_k': 64, 'C': 1.5, 'batch_size': 48, 'num_iterations': 5, 'num_selfPlay_iterations': 50, 'num_epochs': 10, 'beta': 2, 'kl_threshold': 0.5, 'root_dir_eps': 0.15, 'root_dir_alpha': 0.3, 'lr': 0.0002, 'entropy_bonus': 0.02, 'grad_clip': 1.0, 'mcts_batch_size': 128}
[utils] meta-llama/Llama-2-7b-hf:    21748 blocchi da 64 neuroni
PPL baseline: 9.17


modellio compilato e pront! 
--> inizio ad imparare


--- Iteration 0: Self-Play ---
--- Iteration 0: Training (Avg Reward: 4.4735) ---
[Iter 0  Ep 1]  pol=5.1402  val=0.2154  ent=-5.2507
[Iter 0  Ep 2]  pol=5.0069  val=0.1546  ent=-5.2236
[Iter 0  Ep 3]  pol=4.8484  val=0.1642  ent=-5.1650
[Iter 0  Ep 4]  pol=4.8159  val=0.1643  ent=-5.1116
[Iter 0  Ep 5]  pol=4.6930  val=0.1735  ent=-5.0506
[Iter 0  Ep 6]  pol=4.7484  val=0.1567  ent=-5.0297
[Iter 0  Ep 7]  pol=4.7326  val=0.1362  ent=-4.9904
[Iter 0  Ep 8]  pol=4.6096  val=0.1589  ent=-4.9473
[Iter 0  Ep 9]  pol=4.6439  val=0.1426  ent=-4.9628
[Iter 0  Ep 10]  pol=4.5642  val=0.1601  ent=-4.9347

--- Iteration 1: Self-Play ---
--- Iteration 1: Training (Avg Reward: 1.7381) ---
[Iter 1  Ep 1]  pol=4.5245  val=0.3779  ent=-4.9556
[Iter 1  Ep 2]  pol=4.5280  val=0.3250  ent=-4.9409
[Iter 1  Ep 3]  pol=4.5403  val=0.2481  ent=-4.9423
[Iter 1  Ep 4]  pol=4.5341  val=0.2186  ent=-4.9411
[Iter 1  Ep 5]  pol=4.5080  val=0.1759  ent=-4.9214
[Iter 1  Ep 6]  pol=4.5025  val=0.1602  ent=-4.9142
[Iter 1  Ep 7]  pol=4.4990  val=0.1189  ent=-4.8874
[Iter 1  Ep 8]  pol=4.4324  val=0.1067  ent=-4.8372
[Iter 1  Ep 9]  pol=4.4131  val=0.0819  ent=-4.8000
[Iter 1  Ep 10]  pol=4.3830  val=0.1102  ent=-4.7457

--- Iteration 2: Self-Play ---
--- Iteration 2: Training (Avg Reward: 1.9406) ---
[Iter 2  Ep 1]  pol=4.3508  val=0.0936  ent=-4.6903
[Iter 2  Ep 2]  pol=4.3276  val=0.0931  ent=-4.6407
[Iter 2  Ep 3]  pol=4.2978  val=0.1111  ent=-4.6006
[Iter 2  Ep 4]  pol=4.3043  val=0.0510  ent=-4.5665
[Iter 2  Ep 5]  pol=4.2629  val=0.0808  ent=-4.5182
[Iter 2  Ep 6]  pol=4.3472  val=0.0474  ent=-4.6053
[Iter 2  Ep 7]  pol=4.2903  val=0.0968  ent=-4.5333
[Iter 2  Ep 8]  pol=4.2515  val=0.0578  ent=-4.4738
[Iter 2  Ep 9]  pol=4.2735  val=0.0738  ent=-4.5105
[Iter 2  Ep 10]  pol=4.2845  val=0.1187  ent=-4.5163

--- Iteration 3: Self-Play ---
--- Iteration 3: Training (Avg Reward: 2.5063) ---
[Iter 3  Ep 1]  pol=4.2400  val=0.0772  ent=-4.4692
[Iter 3  Ep 2]  pol=4.2559  val=0.0481  ent=-4.4951
[Iter 3  Ep 3]  pol=4.2650  val=0.0573  ent=-4.5060
[Iter 3  Ep 4]  pol=4.2671  val=0.0367  ent=-4.4991
[Iter 3  Ep 5]  pol=4.2562  val=0.0558  ent=-4.4806
[Iter 3  Ep 6]  pol=4.2339  val=0.0751  ent=-4.4572
[Iter 3  Ep 7]  pol=4.2350  val=0.0649  ent=-4.4585
[Iter 3  Ep 8]  pol=4.2659  val=0.0494  ent=-4.4845
[Iter 3  Ep 9]  pol=4.2155  val=0.0476  ent=-4.4052
[Iter 3  Ep 10]  pol=4.2412  val=0.0632  ent=-4.4236

--- Iteration 4: Self-Play ---
--- Iteration 4: Training (Avg Reward: 2.2782) ---
[Iter 4  Ep 1]  pol=4.2439  val=0.1111  ent=-4.4090
[Iter 4  Ep 2]  pol=4.2486  val=0.1507  ent=-4.4059
[Iter 4  Ep 3]  pol=4.2350  val=0.1195  ent=-4.3972
[Iter 4  Ep 4]  pol=4.2034  val=0.1402  ent=-4.3444
[Iter 4  Ep 5]  pol=4.2064  val=0.2464  ent=-4.3464
[Iter 4  Ep 6]  pol=4.2142  val=0.1459  ent=-4.3611
[Iter 4  Ep 7]  pol=4.1993  val=0.1079  ent=-4.3380
[Iter 4  Ep 8]  pol=4.1958  val=0.2028  ent=-4.3081
[Iter 4  Ep 9]  pol=4.1935  val=0.1031  ent=-4.3149
[Iter 4  Ep 10]  pol=4.2062  val=0.0728  ent=-4.3385

ðŸ“ˆ Curva di training salvata in â†’ loss_curve.png


PPL modello potato: 11.03
Sparsity finale   : 6.25%
ðŸ”–  plot salvato in â†’ final_gate_state.png
