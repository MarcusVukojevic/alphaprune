Inizio traning con:  {'name_model': 'meta-llama/Llama-2-7b-hf', 'eightbit': False, 'name_dataset': 'wikitext', 'device': 'cuda', 'target_sparsity': 0.5, 'ppl_tolerance_frac': 0.2, 'beta': 0.8, 'R_limit': 60, 'num_searches': 32, 'top_k': 32, 'C': 1.5, 'batch_size': 48, 'num_iterations': 10, 'num_selfPlay_iterations': 100, 'num_epochs': 10, 'kl_threshold': 0.5, 'root_dir_eps': 0.15, 'root_dir_alpha': 0.3, 'lr': 0.0002, 'entropy_bonus': 0.02, 'grad_clip': 1.0, 'mcts_batch_size': 16}
[utils] meta-llama/Llama-2-7b-hf:    21748 blocchi da 64 neuroni
PPL baseline: 9.17


modellio compilato e pront! 
--> inizio ad imparare


--- Iteration 0: Self-Play ---
--- Iteration 0: Training (Avg Reward: -0.5152) ---
[Iter 0  Ep 1]  pol=4.8867  val=35.9080  ent=3.5777
[Iter 0  Ep 2]  pol=5.2304  val=5.6789  ent=3.7781
[Iter 0  Ep 3]  pol=5.2279  val=4.8223  ent=3.7491
[Iter 0  Ep 4]  pol=4.8609  val=5.0133  ent=3.6637
[Iter 0  Ep 5]  pol=5.2571  val=3.8435  ent=3.5863
[Iter 0  Ep 6]  pol=4.7344  val=1.9320  ent=3.6156
[Iter 0  Ep 7]  pol=4.9106  val=1.5834  ent=3.5776
[Iter 0  Ep 8]  pol=4.7606  val=1.1724  ent=3.5438
[Iter 0  Ep 9]  pol=4.8667  val=1.6022  ent=3.5520
[Iter 0  Ep 10]  pol=4.9383  val=0.9385  ent=3.5281

--- Iteration 1: Self-Play ---
--- Iteration 1: Training (Avg Reward: 0.9174) ---
[Iter 1  Ep 1]  pol=4.8845  val=4.3640  ent=3.5491
[Iter 1  Ep 2]  pol=4.8358  val=3.5980  ent=3.5402
[Iter 1  Ep 3]  pol=4.8848  val=2.8651  ent=3.5363
[Iter 1  Ep 4]  pol=4.7909  val=1.8969  ent=3.5451
[Iter 1  Ep 5]  pol=4.7509  val=1.5886  ent=3.5793
[Iter 1  Ep 6]  pol=4.7677  val=1.0353  ent=3.5522
[Iter 1  Ep 7]  pol=4.6770  val=2.2525  ent=3.5389
[Iter 1  Ep 8]  pol=4.7257  val=3.2834  ent=3.5370
[Iter 1  Ep 9]  pol=4.8461  val=5.8597  ent=3.5172
[Iter 1  Ep 10]  pol=4.8607  val=5.1994  ent=3.6041

--- Iteration 2: Self-Play ---
--- Iteration 2: Training (Avg Reward: -0.4021) ---
[Iter 2  Ep 1]  pol=4.5813  val=10.5693  ent=3.5190
[Iter 2  Ep 2]  pol=4.4791  val=7.8463  ent=3.5763
[Iter 2  Ep 3]  pol=4.5733  val=4.1333  ent=3.4940
[Iter 2  Ep 4]  pol=4.9075  val=2.3255  ent=3.5498
[Iter 2  Ep 5]  pol=4.9985  val=3.4697  ent=3.5162
[Iter 2  Ep 6]  pol=4.7169  val=3.6125  ent=3.5061
[Iter 2  Ep 7]  pol=4.9285  val=3.5559  ent=3.4824
[Iter 2  Ep 8]  pol=4.9266  val=3.4719  ent=3.5737
[Iter 2  Ep 9]  pol=5.0976  val=3.6424  ent=3.5105
[Iter 2  Ep 10]  pol=4.8074  val=3.6515  ent=3.5904

--- Iteration 3: Self-Play ---
--- Iteration 3: Training (Avg Reward: 1.0524) ---
[Iter 3  Ep 1]  pol=5.0110  val=9.1984  ent=3.5512
[Iter 3  Ep 2]  pol=4.8953  val=9.4317  ent=3.5534
[Iter 3  Ep 3]  pol=4.7303  val=9.2068  ent=3.6012
[Iter 3  Ep 4]  pol=5.0271  val=7.6305  ent=3.4855
[Iter 3  Ep 5]  pol=4.7815  val=5.7462  ent=3.5197
[Iter 3  Ep 6]  pol=4.6569  val=3.8867  ent=3.5910
[Iter 3  Ep 7]  pol=4.8361  val=1.7392  ent=3.5974
[Iter 3  Ep 8]  pol=4.7738  val=1.6217  ent=3.4977
[Iter 3  Ep 9]  pol=4.7159  val=3.5491  ent=3.5347
[Iter 3  Ep 10]  pol=4.8397  val=6.7769  ent=3.5671

--- Iteration 4: Self-Play ---
--- Iteration 4: Training (Avg Reward: -0.5171) ---
[Iter 4  Ep 1]  pol=4.9917  val=23.2513  ent=3.5600
[Iter 4  Ep 2]  pol=5.0281  val=28.0997  ent=3.5467
[Iter 4  Ep 3]  pol=4.7019  val=27.5026  ent=3.5609
[Iter 4  Ep 4]  pol=4.4050  val=23.6810  ent=3.5714
[Iter 4  Ep 5]  pol=4.7865  val=19.2103  ent=3.6162
[Iter 4  Ep 6]  pol=4.9189  val=15.3064  ent=3.6079
[Iter 4  Ep 7]  pol=4.5039  val=10.2354  ent=3.5821
[Iter 4  Ep 8]  pol=4.8131  val=7.9065  ent=3.5946
[Iter 4  Ep 9]  pol=4.6541  val=7.2557  ent=3.5803
[Iter 4  Ep 10]  pol=4.7944  val=6.9589  ent=3.5935

--- Iteration 5: Self-Play ---
--- Iteration 5: Training (Avg Reward: -0.7309) ---
[Iter 5  Ep 1]  pol=4.8979  val=5.1815  ent=3.5592
[Iter 5  Ep 2]  pol=4.5779  val=5.2010  ent=3.6177
[Iter 5  Ep 3]  pol=4.9316  val=4.8362  ent=3.5581
[Iter 5  Ep 4]  pol=4.9075  val=4.1260  ent=3.5765
[Iter 5  Ep 5]  pol=4.7315  val=3.1724  ent=3.5711
[Iter 5  Ep 6]  pol=4.7310  val=1.0692  ent=3.5721
[Iter 5  Ep 7]  pol=4.8036  val=0.8847  ent=3.5921
[Iter 5  Ep 8]  pol=4.6549  val=2.2078  ent=3.5975
[Iter 5  Ep 9]  pol=4.9452  val=4.6402  ent=3.6908
[Iter 5  Ep 10]  pol=4.9097  val=8.2220  ent=3.5988

--- Iteration 6: Self-Play ---
--- Iteration 6: Training (Avg Reward: -0.7125) ---
[Iter 6  Ep 1]  pol=4.8177  val=9.3378  ent=3.6088
[Iter 6  Ep 2]  pol=4.7407  val=9.5688  ent=3.5829
[Iter 6  Ep 3]  pol=4.6235  val=8.3023  ent=3.6359
[Iter 6  Ep 4]  pol=4.7736  val=7.4810  ent=3.5931
[Iter 6  Ep 5]  pol=4.7417  val=4.6598  ent=3.6143
[Iter 6  Ep 6]  pol=4.6491  val=2.8291  ent=3.6210
[Iter 6  Ep 7]  pol=4.9367  val=1.7956  ent=3.6488
[Iter 6  Ep 8]  pol=4.5140  val=1.0708  ent=3.5681
[Iter 6  Ep 9]  pol=4.6153  val=0.7001  ent=3.6236
[Iter 6  Ep 10]  pol=4.5893  val=0.7878  ent=3.6102

--- Iteration 7: Self-Play ---
--- Iteration 7: Training (Avg Reward: -0.6324) ---
[Iter 7  Ep 1]  pol=4.9080  val=0.5955  ent=3.6231
[Iter 7  Ep 2]  pol=4.7469  val=1.0049  ent=3.6086
[Iter 7  Ep 3]  pol=4.8003  val=0.8166  ent=3.6176
[Iter 7  Ep 4]  pol=4.7692  val=0.7390  ent=3.5930
[Iter 7  Ep 5]  pol=4.7692  val=1.0723  ent=3.6277
[Iter 7  Ep 6]  pol=4.6328  val=0.5788  ent=3.6402
[Iter 7  Ep 7]  pol=4.7781  val=1.1578  ent=3.6550
[Iter 7  Ep 8]  pol=4.5392  val=2.2444  ent=3.6001
[Iter 7  Ep 9]  pol=4.7570  val=2.9087  ent=3.6656
[Iter 7  Ep 10]  pol=4.8766  val=3.4124  ent=3.6142

--- Iteration 8: Self-Play ---
--- Iteration 8: Training (Avg Reward: -0.6023) ---
[Iter 8  Ep 1]  pol=4.3504  val=2.6885  ent=3.6259
[Iter 8  Ep 2]  pol=4.9818  val=1.4539  ent=3.6337
[Iter 8  Ep 3]  pol=4.7440  val=1.2016  ent=3.6594
[Iter 8  Ep 4]  pol=4.6691  val=1.0608  ent=3.6559
[Iter 8  Ep 5]  pol=4.6768  val=1.5260  ent=3.6989
[Iter 8  Ep 6]  pol=4.5299  val=1.2716  ent=3.6600
[Iter 8  Ep 7]  pol=4.6301  val=1.2938  ent=3.6736
[Iter 8  Ep 8]  pol=4.7421  val=0.8654  ent=3.6800
[Iter 8  Ep 9]  pol=4.6326  val=0.7043  ent=3.6602
[Iter 8  Ep 10]  pol=4.7348  val=0.6868  ent=3.6523

--- Iteration 9: Self-Play ---
--- Iteration 9: Training (Avg Reward: -0.6463) ---
[Iter 9  Ep 1]  pol=4.6807  val=0.7172  ent=3.6858
[Iter 9  Ep 2]  pol=4.4223  val=0.6787  ent=3.6764
[Iter 9  Ep 3]  pol=4.4844  val=0.8495  ent=3.6608
[Iter 9  Ep 4]  pol=4.6657  val=0.4843  ent=3.6923
[Iter 9  Ep 5]  pol=4.6805  val=0.7138  ent=3.7334
[Iter 9  Ep 6]  pol=4.5336  val=0.6455  ent=3.6722
[Iter 9  Ep 7]  pol=4.7406  val=0.8394  ent=3.6978
[Iter 9  Ep 8]  pol=4.7292  val=1.0721  ent=3.6832
[Iter 9  Ep 9]  pol=4.7907  val=1.0813  ent=3.7394
[Iter 9  Ep 10]  pol=4.6162  val=1.0202  ent=3.7624

ðŸ“ˆ Curva di training salvata in â†’ loss_curve.png


PPL modello potato: 275.58
Sparsity finale   : 28.12%
ðŸ”–  plot salvato in â†’ final_gate_state.png
