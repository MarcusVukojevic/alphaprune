Inizio traning con:  {'name_model': 'distilgpt2', 'eightbit': False, 'name_dataset': 'wikitext', 'device': 'cuda', 'target_sparsity': 0.4, 'ppl_tolerance_frac': 0.2, 'beta': 0.5, 'R_limit': 30, 'num_searches': 32, 'top_k': 32, 'C': 1.5, 'batch_size': 16, 'num_iterations': 4, 'num_selfPlay_iterations': 30, 'num_epochs': 5, 'kl_threshold': 0.5, 'root_dir_eps': 0.3, 'root_dir_alpha': 0.3}
[utils] distilgpt2:    786 blocchi da 64 neuroni
[utils] text dataset ('wikitext', 'wikitext-2-raw-v1') â€“ 10 sequenze da 128
PPL baseline: 1441.50


--> inizio ad imparare

[Iter 0  Ep 1]  pol=4.1925  val=10.5245  tot=14.7170
[Iter 0  Ep 2]  pol=4.4246  val=3.8337  tot=8.2583
[Iter 0  Ep 3]  pol=4.3505  val=2.3201  tot=6.6706
[Iter 0  Ep 4]  pol=4.4270  val=1.8648  tot=6.2918
[Iter 0  Ep 5]  pol=4.0781  val=2.7234  tot=6.8015
[Iter 1  Ep 1]  pol=4.3080  val=1.4431  tot=5.7510
[Iter 1  Ep 2]  pol=4.6113  val=0.8391  tot=5.4505
[Iter 1  Ep 3]  pol=4.2272  val=1.6420  tot=5.8691
[Iter 1  Ep 4]  pol=4.4536  val=1.3255  tot=5.7791
[Iter 1  Ep 5]  pol=4.4271  val=2.3323  tot=6.7594
[Iter 2  Ep 1]  pol=3.8812  val=1.5652  tot=5.4464
[Iter 2  Ep 2]  pol=4.4356  val=2.2235  tot=6.6591
[Iter 2  Ep 3]  pol=3.4988  val=1.0732  tot=4.5719
[Iter 2  Ep 4]  pol=4.0580  val=0.6716  tot=4.7297
[Iter 2  Ep 5]  pol=4.3293  val=1.3032  tot=5.6325
[Iter 3  Ep 1]  pol=4.5172  val=0.9912  tot=5.5084
[Iter 3  Ep 2]  pol=3.8954  val=1.2737  tot=5.1691
[Iter 3  Ep 3]  pol=4.9319  val=1.8279  tot=6.7597
[Iter 3  Ep 4]  pol=4.3467  val=1.2605  tot=5.6072
[Iter 3  Ep 5]  pol=4.1078  val=1.0683  tot=5.1760

ðŸ“ˆ  curva loss salvata in â†’  loss_curve.png


PPL modello potato: 33533.91
ðŸ”–  plot salvato in â†’ final_gate_state.png
